"""
Agent Synth√©tiseur Tech avec LangGraph - G√©n√©rateur de Digest Quotidien.

Cet agent transforme les articles analys√©s en digest quotidien structur√©
avec r√©sum√© ex√©cutif, insights cl√©s et recommandations actionables.
"""
import asyncio
import json
import time
from typing import List, Dict, Any, Optional
from datetime import datetime, timedelta
from dataclasses import asdict

from langchain_core.messages import HumanMessage, SystemMessage
from langchain_openai import ChatOpenAI
from langgraph.graph import StateGraph, START, END
from langchain_core.runnables import RunnableConfig
from loguru import logger
from dotenv import load_dotenv
import os

# Chargement automatique des variables d'environnement
load_dotenv()

# Imports des mod√®les
from ..agents.tech_analyzer_agent import AnalyzedContent
from ..models.synthesis_models import (
    SynthesisState, SynthesisStage, ReportSection,
    ArticleSynthesis, ActionableRecommendation, TechnicalTrend, DailyDigest,
    DEFAULT_SYNTHESIS_CONFIG, SYNTHESIS_PROMPTS
)


class TechSynthesizerAgent:
    """
    Agent Synth√©tiseur Tech avec LangGraph.
    
    Workflow intelligent pour cr√©er des digests quotidiens √† partir
    d'articles analys√©s avec :
    - Synth√®se ex√©cutive automatis√©e
    - Extraction d'insights transversaux
    - G√©n√©ration de recommandations actionables
    - Formatage Markdown professionnel
    """
    
    def __init__(self, config: Dict[str, Any] = None):
        """
        Initialise l'agent synth√©tiseur.
        
        Args:
            config: Configuration de synth√®se (utilise la config par d√©faut si None)
        """
        self.config = config or DEFAULT_SYNTHESIS_CONFIG.copy()
        self.logger = logger.bind(component="TechSynthesizerAgent")
        
        # Configuration LLM - utilise GPT-4o pour la synth√®se qualitative
        self.llm = ChatOpenAI(
            model="gpt-4o",
            temperature=0.2,  # L√©g√®rement plus cr√©atif pour la synth√®se
            max_tokens=1000,
            api_key=os.getenv('OPENAI_API_KEY')
        )
        
        # Construction du workflow LangGraph
        self.workflow = self._build_workflow()
        self.compiled_workflow = self.workflow.compile()
        
        self.logger.info("üìù Agent Synth√©tiseur Tech (LangGraph) initialis√©")
    
    def _build_workflow(self) -> StateGraph:
        """Construit le workflow LangGraph pour la synth√®se."""
        
        # Cr√©ation du graphe d'√©tat
        workflow = StateGraph(SynthesisState)
        
        # Ajout des n≈ìuds
        workflow.add_node("prepare_content", self._prepare_synthesis)
        workflow.add_node("generate_summary", self._generate_executive_summary)
        workflow.add_node("synthesize_articles", self._synthesize_articles)
        workflow.add_node("extract_insights", self._extract_key_insights)
        workflow.add_node("create_recommendations", self._create_recommendations)
        workflow.add_node("format_digest", self._format_markdown_digest)
        workflow.add_node("finalize", self._finalize_synthesis)
        
        # D√©finition des ar√™tes - workflow s√©quentiel
        workflow.set_entry_point("prepare_content")
        
        workflow.add_edge("prepare_content", "generate_summary")
        workflow.add_edge("generate_summary", "synthesize_articles")
        workflow.add_edge("synthesize_articles", "extract_insights")
        workflow.add_edge("extract_insights", "create_recommendations")
        workflow.add_edge("create_recommendations", "format_digest")
        workflow.add_edge("format_digest", "finalize")
        workflow.add_edge("finalize", END)
        
        return workflow
    
    async def create_daily_digest(
        self, 
        analyzed_articles: List[AnalyzedContent],
        config: Optional[RunnableConfig] = None
    ) -> DailyDigest:
        """
        Point d'entr√©e principal pour cr√©er un digest quotidien.
        
        Args:
            analyzed_articles: Articles analys√©s depuis l'Agent Analyseur
            config: Configuration LangGraph optionnelle
            
        Returns:
            Digest quotidien complet avec contenu Markdown
        """
        if not analyzed_articles:
            raise ValueError("Aucun article analys√© fourni pour la synth√®se")
        
        self.logger.info(f"üìù D√©but cr√©ation digest de {len(analyzed_articles)} articles")
        
        # Filtrage des articles recommand√©s pour le digest
        recommended_articles = [
            article for article in analyzed_articles 
            if article.analysis.recommended
        ][:self.config["max_articles_in_digest"]]
        
        if not recommended_articles:
            self.logger.warning("Aucun article recommand√© trouv√©, utilisation des top articles")
            recommended_articles = analyzed_articles[:self.config["max_articles_in_digest"]]
        
        # √âtat initial
        initial_state: SynthesisState = {
            "synthesis_config": self.config,
            "target_audience": self.config["target_audience"],
            "analyzed_articles": recommended_articles,
            "current_stage": SynthesisStage.INITIAL,
            "processed_sections": [],
            "executive_summary": None,
            "articles_synthesis": [],
            "key_insights": [],
            "technical_trends": [],
            "recommendations": [],
            "final_digest": None,
            "generation_time": 0.0,
            "word_count": 0,
            "errors": []
        }
        
        # Ex√©cution du workflow
        try:
            start_time = time.time()
            final_state = await self.compiled_workflow.ainvoke(
                initial_state,
                config=config or {}
            )
            
            # Extraction du digest final
            digest = final_state.get("final_digest")
            if not digest:
                raise RuntimeError("√âchec de la g√©n√©ration du digest")
            
            # Calcul du temps de g√©n√©ration
            generation_time = time.time() - start_time
            digest.generation_time = generation_time
            
            # Logging des r√©sultats
            self.logger.info(f"‚úÖ Digest cr√©√© en {generation_time:.2f}s")
            self.logger.info(f"üìä {len(digest.top_articles)} articles, {digest.word_count} mots")
            self.logger.info(f"üéØ {len(digest.recommendations)} recommandations")
            
            if final_state.get("errors"):
                self.logger.warning(f"‚ö†Ô∏è {len(final_state['errors'])} erreurs pendant la g√©n√©ration")
            
            return digest
            
        except Exception as e:
            self.logger.error(f"‚ùå Erreur workflow synth√®se: {e}")
            raise
    
    async def _prepare_synthesis(self, state: SynthesisState) -> SynthesisState:
        """N≈ìud de pr√©paration du contenu pour la synth√®se."""
        self.logger.debug("üîÑ Pr√©paration du contenu pour synth√®se")
        
        # Validation des articles
        articles = state["analyzed_articles"]
        if not articles:
            state["errors"].append("Aucun article √† synth√©tiser")
            return state
        
        # Tri par score d√©croissant pour garantir la qualit√©
        sorted_articles = sorted(
            articles, 
            key=lambda x: x.analysis.relevance_score, 
            reverse=True
        )
        
        # Limitation au nombre configur√©
        max_articles = state["synthesis_config"]["max_articles_in_digest"]
        final_articles = sorted_articles[:max_articles]
        
        self.logger.info(f"üìã Pr√©paration: {len(final_articles)} articles s√©lectionn√©s")
        
        return {
            "current_stage": SynthesisStage.PREPARED,
            "analyzed_articles": final_articles,
            "processed_sections": [ReportSection.TOP_ARTICLES]
        }
    
    async def _generate_executive_summary(self, state: SynthesisState) -> SynthesisState:
        """G√©n√®re le r√©sum√© ex√©cutif du digest."""
        self.logger.debug("üìÑ G√©n√©ration r√©sum√© ex√©cutif")
        
        articles = state["analyzed_articles"]
        
        # Pr√©paration du contexte pour le prompt
        articles_overview = "\n".join([
            f"- {article.raw_content.title} (Score: {article.analysis.relevance_score:.2f}, {article.analysis.category})"
            for article in articles
        ])
        
        # Construction du prompt
        prompt_content = SYNTHESIS_PROMPTS["executive_summary"].format(
            target_audience=state["target_audience"],
            articles_count=len(articles),
            articles_overview=articles_overview
        )
        
        try:
            # Appel LLM
            messages = [
                SystemMessage(content="Tu es un expert technique qui r√©dige des synth√®ses de veille."),
                HumanMessage(content=prompt_content)
            ]
            
            response = await self.llm.ainvoke(messages)
            executive_summary = response.content.strip()
            
            self.logger.debug(f"‚úÖ R√©sum√© ex√©cutif g√©n√©r√© ({len(executive_summary)} caract√®res)")
            
            return {
                "current_stage": SynthesisStage.SUMMARIZED,
                "executive_summary": executive_summary,
                "processed_sections": state["processed_sections"] + [ReportSection.EXECUTIVE_SUMMARY]
            }
            
        except Exception as e:
            error_msg = f"Erreur g√©n√©ration r√©sum√© ex√©cutif: {str(e)}"
            self.logger.error(error_msg)
            state["errors"].append(error_msg)
            
            # Fallback
            return {
                "executive_summary": "R√©sum√© ex√©cutif indisponible (erreur de g√©n√©ration)",
                "current_stage": SynthesisStage.SUMMARIZED
            }
    
    async def _synthesize_articles(self, state: SynthesisState) -> SynthesisState:
        """Synth√©tise chaque article individuellement."""
        self.logger.debug(f"üìù Synth√®se de {len(state['analyzed_articles'])} articles")
        
        articles_synthesis = []
        
        for article in state["analyzed_articles"]:
            try:
                synthesis = await self._synthesize_single_article(article)
                articles_synthesis.append(synthesis)
                
                self.logger.debug(f"‚úÖ Article synth√©tis√©: {synthesis.title_refined[:50]}...")
                
            except Exception as e:
                error_msg = f"Erreur synth√®se article {article.raw_content.title[:30]}...: {str(e)}"
                self.logger.error(error_msg)
                state["errors"].append(error_msg)
                
                # Cr√©ation d'une synth√®se de fallback
                fallback_synthesis = ArticleSynthesis(
                    original_article=article,
                    title_refined=article.raw_content.title,
                    executive_summary="Synth√®se indisponible (erreur de g√©n√©ration)",
                    key_takeaways=["Contenu technique √† analyser manuellement"],
                    technical_highlights=["D√©tails d'impl√©mentation √† examiner"],
                    relevance_for_audience=article.analysis.relevance_score,
                    actionability_score=0.5,
                    innovation_level="unknown",
                    estimated_read_time=10,
                    complexity_level=article.analysis.expertise_level
                )
                articles_synthesis.append(fallback_synthesis)
        
        return {
            "current_stage": SynthesisStage.INSIGHTS_EXTRACTED,
            "articles_synthesis": articles_synthesis
        }
    
    async def _synthesize_single_article(self, article: AnalyzedContent) -> ArticleSynthesis:
        """Synth√©tise un article unique avec le LLM."""
        
        # Pr√©paration du prompt
        prompt_content = SYNTHESIS_PROMPTS["article_synthesis"].format(
            title=article.raw_content.title,
            source=article.raw_content.source,
            category=article.analysis.category,
            score=article.analysis.relevance_score,
            insights=', '.join(article.analysis.key_insights) if article.analysis.key_insights else "N/A",
            content=article.raw_content.content[:2000] if article.raw_content.content else article.raw_content.excerpt or "Contenu non disponible"
        )
        
        messages = [
            SystemMessage(content="Tu es un expert qui synth√©tise des articles techniques pour des ing√©nieurs seniors."),
            HumanMessage(content=prompt_content)
        ]
        
        response = await self.llm.ainvoke(messages)
        
        # Parse de la r√©ponse JSON
        try:
            result_data = json.loads(response.content)
            
            return ArticleSynthesis(
                original_article=article,
                title_refined=result_data.get("title_refined", article.raw_content.title),
                executive_summary=result_data.get("executive_summary", ""),
                key_takeaways=result_data.get("key_takeaways", []),
                technical_highlights=result_data.get("technical_highlights", []),
                relevance_for_audience=article.analysis.relevance_score,
                actionability_score=article.analysis.practical_value,
                innovation_level=result_data.get("innovation_level", "incremental"),
                estimated_read_time=max(5, len(article.raw_content.content or "") // 200),
                complexity_level=result_data.get("complexity_level", article.analysis.expertise_level)
            )
            
        except (json.JSONDecodeError, KeyError) as e:
            self.logger.warning(f"Erreur parsing synth√®se article: {e}")
            # Retourne une synth√®se basique
            return ArticleSynthesis(
                original_article=article,
                title_refined=article.raw_content.title,
                executive_summary=article.analysis.key_insights or "Article technique √† analyser",
                key_takeaways=["Contenu technique pertinent"],
                technical_highlights=["Impl√©mentation et architecture"],
                relevance_for_audience=article.analysis.relevance_score,
                actionability_score=article.analysis.practical_value,
                innovation_level="incremental",
                estimated_read_time=10,
                complexity_level=article.analysis.expertise_level
            )
    
    async def _extract_key_insights(self, state: SynthesisState) -> SynthesisState:
        """Extrait les insights cl√©s transversaux."""
        self.logger.debug("üîç Extraction des insights cl√©s")
        
        # Pr√©paration du contexte depuis les synth√®ses d'articles
        articles_summaries = "\n\n".join([
            f"ARTICLE: {synthesis.title_refined}\n"
            f"R√©sum√©: {synthesis.executive_summary}\n"
            f"Points cl√©s: {', '.join(synthesis.key_takeaways)}\n"
            f"Aspects techniques: {', '.join(synthesis.technical_highlights)}"
            for synthesis in state["articles_synthesis"]
        ])
        
        prompt_content = SYNTHESIS_PROMPTS["insights_extraction"].format(
            articles_summaries=articles_summaries
        )
        
        try:
            messages = [
                SystemMessage(content="Tu es un expert en veille technologique qui identifie les tendances √©mergentes."),
                HumanMessage(content=prompt_content)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Parse des insights (un par ligne)
            insights_text = response.content.strip()
            insights = [
                insight.strip().lstrip('- ').lstrip('* ') 
                for insight in insights_text.split('\n') 
                if insight.strip() and not insight.strip().startswith('#')
            ]
            
            # Limitation au nombre configur√©
            max_insights = state["synthesis_config"]["max_insights"]
            final_insights = insights[:max_insights]
            
            self.logger.debug(f"‚úÖ {len(final_insights)} insights extraits")
            
            return {
                "current_stage": SynthesisStage.INSIGHTS_EXTRACTED,
                "key_insights": final_insights,
                "processed_sections": state["processed_sections"] + [ReportSection.KEY_INSIGHTS]
            }
            
        except Exception as e:
            error_msg = f"Erreur extraction insights: {str(e)}"
            self.logger.error(error_msg)
            state["errors"].append(error_msg)
            
            # Fallback
            return {
                "key_insights": ["Analyse des tendances indisponible"],
                "current_stage": SynthesisStage.INSIGHTS_EXTRACTED
            }
    
    async def _create_recommendations(self, state: SynthesisState) -> SynthesisState:
        """Cr√©e les recommandations actionables."""
        self.logger.debug("üéØ Cr√©ation des recommandations actionables")
        
        # Pr√©paration du contexte avec articles et insights
        content_summary = f"""ARTICLES SYNTH√âTIS√âS:
{chr(10).join([f"- {s.title_refined}: {s.executive_summary}" for s in state["articles_synthesis"]])}

INSIGHTS CL√âS:
{chr(10).join([f"- {insight}" for insight in state["key_insights"]])}"""
        
        prompt_content = SYNTHESIS_PROMPTS["recommendations"].format(
            content_summary=content_summary,
            target_audience=state["target_audience"]
        )
        
        try:
            messages = [
                SystemMessage(content="Tu es un tech lead qui transforme la veille en actions concr√®tes."),
                HumanMessage(content=prompt_content)
            ]
            
            response = await self.llm.ainvoke(messages)
            
            # Parse des recommandations JSON
            result_data = json.loads(response.content)
            recommendations_data = result_data.get("recommendations", [])
            
            recommendations = []
            for rec_data in recommendations_data:
                recommendation = ActionableRecommendation(
                    title=rec_data.get("title", "Recommandation"),
                    description=rec_data.get("description", ""),
                    action_items=rec_data.get("action_items", []),
                    based_on_articles=[article.raw_content.url for article in state["analyzed_articles"]],
                    category=rec_data.get("category", "investigation"),
                    priority=rec_data.get("priority", "medium"),
                    time_investment=rec_data.get("time_investment", "1-4h"),
                    complexity="intermediate",
                    suggested_next_steps=[],
                    related_technologies=[]
                )
                recommendations.append(recommendation)
            
            # Limitation au nombre configur√©
            max_recommendations = state["synthesis_config"]["max_recommendations"]
            final_recommendations = recommendations[:max_recommendations]
            
            self.logger.debug(f"‚úÖ {len(final_recommendations)} recommandations cr√©√©es")
            
            return {
                "current_stage": SynthesisStage.RECOMMENDATIONS_CREATED,
                "recommendations": final_recommendations,
                "processed_sections": state["processed_sections"] + [ReportSection.RECOMMENDATIONS]
            }
            
        except (json.JSONDecodeError, KeyError, Exception) as e:
            error_msg = f"Erreur cr√©ation recommandations: {str(e)}"
            self.logger.error(error_msg)
            state["errors"].append(error_msg)
            
            # Fallback avec recommandations basiques
            fallback_recommendation = ActionableRecommendation(
                title="Approfondir les technologies √©mergentes",
                description="Explorer les innovations identifi√©es dans la veille",
                action_items=["Lire les articles s√©lectionn√©s", "√âvaluer l'impact sur vos projets"],
                based_on_articles=[],
                category="learning",
                priority="medium",
                time_investment="1-4h",
                complexity="intermediate",
                suggested_next_steps=[],
                related_technologies=[]
            )
            
            return {
                "recommendations": [fallback_recommendation],
                "current_stage": SynthesisStage.RECOMMENDATIONS_CREATED
            }
    
    async def _format_markdown_digest(self, state: SynthesisState) -> SynthesisState:
        """Formate le digest final en Markdown."""
        self.logger.debug("üìã Formatage du digest Markdown")
        
        try:
            # Construction du digest final
            digest = DailyDigest(
                date=datetime.now(),
                title=f"Tech Digest - {datetime.now().strftime('%d %B %Y')}",
                subtitle="Veille technologique GenAI/LLM/Agentic pour ing√©nieurs seniors",
                target_audience=state["target_audience"],
                executive_summary=state["executive_summary"] or "R√©sum√© non disponible",
                top_articles=state["articles_synthesis"],
                key_insights=state["key_insights"],
                technical_trends=[],  # Simplifi√© pour cette version
                recommendations=state["recommendations"],
                total_articles_analyzed=len(state["analyzed_articles"]),
                articles_recommended=len([a for a in state["analyzed_articles"] if a.analysis.recommended]),
                average_quality_score=sum(a.analysis.relevance_score for a in state["analyzed_articles"]) / len(state["analyzed_articles"]),
                all_article_links=[
                    {"title": a.raw_content.title, "url": a.raw_content.url, "source": a.raw_content.source}
                    for a in state["analyzed_articles"]
                ],
                suggested_reading=[],
                generated_at=datetime.now(),
                generator_version="1.0",
                llm_model_used="gpt-4o"
            )
            
            # G√©n√©ration du contenu Markdown
            markdown_content = self._generate_markdown_content(digest)
            digest.markdown_content = markdown_content
            digest.word_count = len(markdown_content.split())
            digest.estimated_read_time = max(1, digest.word_count // 200)  # 200 mots/minute
            
            self.logger.debug(f"‚úÖ Digest format√© ({digest.word_count} mots, {digest.estimated_read_time}min)")
            
            return {
                "current_stage": SynthesisStage.FORMATTED,
                "final_digest": digest,
                "word_count": digest.word_count,
                "processed_sections": state["processed_sections"] + [ReportSection.RESOURCES]
            }
            
        except Exception as e:
            error_msg = f"Erreur formatage digest: {str(e)}"
            self.logger.error(error_msg)
            state["errors"].append(error_msg)
            
            # Digest minimal de fallback
            fallback_digest = DailyDigest(
                date=datetime.now(),
                title="Tech Digest - Erreur de g√©n√©ration",
                subtitle="Erreur lors de la g√©n√©ration du digest",
                target_audience=state["target_audience"],
                executive_summary="Erreur de g√©n√©ration du digest",
                top_articles=[],
                key_insights=[],
                technical_trends=[],
                recommendations=[],
                total_articles_analyzed=0,
                articles_recommended=0,
                average_quality_score=0.0,
                all_article_links=[],
                suggested_reading=[],
                markdown_content="# Erreur\n\n√âchec de la g√©n√©ration du digest.",
                word_count=0,
                estimated_read_time=0
            )
            
            return {
                "final_digest": fallback_digest,
                "current_stage": SynthesisStage.FORMATTED
            }
    
    def _generate_markdown_content(self, digest: DailyDigest) -> str:
        """G√©n√®re le contenu Markdown du digest."""
        
        # En-t√™te
        markdown = f"""# {digest.title}

> {digest.subtitle}  
> üìÖ {digest.date.strftime('%d %B %Y')} ‚Ä¢ üéØ {digest.target_audience} ‚Ä¢ ‚è±Ô∏è {digest.estimated_read_time} min de lecture

---

## üìä R√©sum√© Ex√©cutif

{digest.executive_summary}

**üìà M√©triques de veille:**
- üìÑ **Articles analys√©s:** {digest.total_articles_analyzed}
- ‚úÖ **Articles recommand√©s:** {digest.articles_recommended}
- üéØ **Score moyen de qualit√©:** {digest.average_quality_score:.2f}/1.0

---

## üèÜ Top Articles

"""
        
        # Articles
        for i, article in enumerate(digest.top_articles, 1):
            innovation_emoji = {"breakthrough": "üöÄ", "significant": "üí°", "incremental": "üìà"}.get(article.innovation_level, "üìÑ")
            complexity_emoji = {"expert": "üî¨", "advanced": "üß†", "intermediate": "üìö"}.get(article.complexity_level, "üìñ")
            
            markdown += f"""### {i}. {innovation_emoji} {article.title_refined}

**{complexity_emoji} {article.complexity_level.title()} ‚Ä¢ ‚è±Ô∏è {article.estimated_read_time}min ‚Ä¢ üìä {article.relevance_for_audience:.2f}/1.0**

{article.executive_summary}

**üîë Points cl√©s:**
"""
            for takeaway in article.key_takeaways:
                markdown += f"- {takeaway}\n"
            
            if article.technical_highlights:
                markdown += "\n**‚öôÔ∏è Aspects techniques:**\n"
                for highlight in article.technical_highlights:
                    markdown += f"- {highlight}\n"
            
            markdown += f"\nüîó **Source:** [{article.original_article.raw_content.source}]({article.original_article.raw_content.url})\n\n---\n\n"
        
        # Insights
        if digest.key_insights:
            markdown += "## üí° Insights Cl√©s\n\n"
            for insight in digest.key_insights:
                markdown += f"- **{insight}**\n"
            markdown += "\n---\n\n"
        
        # Recommandations
        if digest.recommendations:
            markdown += "## üéØ Recommandations Actionables\n\n"
            for i, rec in enumerate(digest.recommendations, 1):
                priority_emoji = {"high": "üî•", "medium": "‚ö°", "low": "üìå"}.get(rec.priority, "üìã")
                category_emoji = {"learning": "üìö", "implementation": "‚öôÔ∏è", "investigation": "üîç", "monitoring": "üëÄ"}.get(rec.category, "üìã")
                
                markdown += f"""### {i}. {priority_emoji} {rec.title}

**{category_emoji} {rec.category.title()} ‚Ä¢ ‚è±Ô∏è {rec.time_investment} ‚Ä¢ üéØ {rec.priority.title()} priority**

{rec.description}

**Actions concr√®tes:**
"""
                for action in rec.action_items:
                    markdown += f"- [ ] {action}\n"
                
                markdown += "\n---\n\n"
        
        # Ressources
        markdown += "## üìö Ressources\n\n"
        markdown += "### üîó Liens des articles\n\n"
        for link in digest.all_article_links:
            markdown += f"- [{link['title']}]({link['url']}) *({link['source']})*\n"
        
        # Footer
        markdown += f"""

---

*Digest g√©n√©r√© le {digest.generated_at.strftime('%d/%m/%Y √† %H:%M')} par {digest.generator_version} ‚Ä¢ LLM: {digest.llm_model_used}*
"""
        
        return markdown
    
    async def _finalize_synthesis(self, state: SynthesisState) -> SynthesisState:
        """Finalise la synth√®se."""
        self.logger.debug("‚úÖ Finalisation de la synth√®se")
        
        return {
            "current_stage": SynthesisStage.FINALIZED,
            "generation_time": time.time()
        }
    
    async def save_digest_to_file(self, digest: DailyDigest, output_dir: str = "output/reports") -> str:
        """Sauvegarde le digest dans un fichier Markdown."""
        
        # Cr√©ation du r√©pertoire de sortie
        os.makedirs(output_dir, exist_ok=True)
        
        # Nom du fichier avec date
        filename = f"tech_digest_{digest.date.strftime('%Y%m%d')}.md"
        filepath = os.path.join(output_dir, filename)
        
        # √âcriture du fichier
        with open(filepath, 'w', encoding='utf-8') as f:
            f.write(digest.markdown_content)
        
        self.logger.info(f"üìù Digest sauvegard√©: {filepath}")
        return filepath
