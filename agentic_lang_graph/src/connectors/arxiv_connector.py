"""
Connecteur ArXiv pour la collecte de papers acad√©miques sur GenAI/LLM.

Ce connecteur utilise l'API ArXiv pour collecter des papers r√©cents
sur les sujets de recherche en intelligence artificielle.

ArXiv API :
- API REST publique et gratuite
- Recherche par cat√©gories et mots-cl√©s
- M√©tadonn√©es compl√®tes des papers
- Pas de limite de taux stricte (mais utilisation polie recommand√©e)
"""
import asyncio
import aiohttp
import xml.etree.ElementTree as ET
from datetime import datetime, timezone, timedelta
from typing import List, Optional, Dict, Any
from urllib.parse import quote_plus
from loguru import logger

from .base_connector import BaseConnector, RawContent


class ArxivConnector(BaseConnector):
    """
    Connecteur pour ArXiv utilisant l'API de recherche officielle.
    
    Ce connecteur collecte des papers depuis ArXiv en utilisant :
    1. L'API de recherche ArXiv (http://export.arxiv.org/api/query)
    2. Recherche par cat√©gories pertinentes (cs.AI, cs.CL, cs.LG, etc.)
    3. Recherche par mots-cl√©s dans les titres et abstracts
    
    Avantages de l'API ArXiv :
    - API officielle gratuite et stable
    - M√©tadonn√©es compl√®tes (auteurs, abstract, cat√©gories)
    - Recherche fine par cat√©gories et dates
    - Acc√®s au texte complet (PDF)
    
    Limitations :
    - Contenu acad√©mique uniquement
    - Peut √™tre tr√®s technique
    - Volume variable selon les p√©riodes
    """
    
    # Cat√©gories ArXiv OPTIMIS√âES (plus actives)
    RELEVANT_CATEGORIES = [
        "cs.LG",    # Machine Learning (plus actif)
        "cs.CL",    # Computation and Language (NLP)
        "stat.ML",  # Machine Learning (Statistics)
        "cs.AI",    # Artificial Intelligence
        "cs.CV",    # Computer Vision
        "cs.IR",    # Information Retrieval
        "cs.HC",    # Human-Computer Interaction
        "cs.MA",    # Multiagent Systems
        "cs.NE",    # Neural and Evolutionary Computing
    ]
    
    # Mots-cl√©s OPTIMIS√âS (plus g√©n√©riques et populaires)
    DEFAULT_KEYWORDS = [
        # Mots-cl√©s tr√®s populaires (priorit√© haute)
        "machine learning",
        "neural network", 
        "artificial intelligence",
        "deep learning",
        
        # Mots-cl√©s IA moderne
        "transformer",
        "large language model",
        "language model",
        "LLM",
        
        # Techniques sp√©cifiques
        "attention mechanism",
        "generative AI", 
        "natural language processing",
        "computer vision",
        "reinforcement learning",
        
        # Mod√®les populaires
        "GPT",
        "BERT",
        "transformer model",
        
        # Applications
        "multi-agent",
        "chatbot",
        "conversational AI",
        "prompt engineering",
        "fine-tuning",
        "RLHF",
        "instruction tuning"
    ]
    
    def __init__(self, keywords: List[str] = None, categories: List[str] = None):
        """
        Initialise le connecteur ArXiv.
        
        Args:
            keywords: Mots-cl√©s pour filtrer les papers (en plus des d√©fauts)
            categories: Cat√©gories ArXiv √† surveiller (utilise les d√©fauts si None)
        """
        super().__init__("arxiv", keywords)
        
        # Configuration sp√©cifique √† ArXiv
        self.base_url = "http://export.arxiv.org/api/query"
        self.timeout = 30
        self.user_agent = "Agent-Veille-Tech/1.0 (https://github.com/user/agent-veille)"
        
        # Cat√©gories √† surveiller
        self.categories = categories or self.RELEVANT_CATEGORIES
        
        # Keywords combin√©s (d√©fauts + utilisateur)
        all_keywords = self.DEFAULT_KEYWORDS.copy()
        if keywords:
            all_keywords.extend(keywords)
        self.search_keywords = list(set(all_keywords))  # Supprime les doublons
        
        # Configuration de recherche OPTIMIS√âE
        self.max_results_per_query = 200  # Limite par requ√™te (maximis√©e)
        self.days_back = 180  # Chercher les papers des N derniers jours (tr√®s permissif)
        
        self.logger.info(f"ArXiv connector initialis√© avec {len(self.categories)} cat√©gories et {len(self.search_keywords)} mots-cl√©s")
    
    async def collect(self, limit: int = 10) -> List[RawContent]:
        """
        Collecte les papers ArXiv r√©cents.
        
        Processus :
        1. Construit des requ√™tes de recherche par cat√©gorie et mots-cl√©s
        2. Ex√©cute les requ√™tes en parall√®le
        3. Parse les r√©ponses XML
        4. Filtre et trie les r√©sultats
        5. Retourne les plus pertinents
        
        Args:
            limit: Nombre maximum de papers √† retourner
            
        Returns:
            Liste de papers ArXiv format√©s en RawContent
        """
        self.logger.info(f"üîç D√©but collecte ArXiv (limite: {limit})")
        
        all_contents = []
        
        # Configuration pour les requ√™tes HTTP
        timeout = aiohttp.ClientTimeout(total=self.timeout)
        headers = {"User-Agent": self.user_agent}
        
        async with aiohttp.ClientSession(timeout=timeout, headers=headers) as session:
            # G√©n√®re les requ√™tes de recherche
            search_queries = self._build_search_queries()
            
            # Ex√©cute toutes les requ√™tes en parall√®le
            tasks = [
                self._execute_search(session, query, self.max_results_per_query) 
                for query in search_queries
            ]
            
            # Attend toutes les r√©ponses
            search_results = await asyncio.gather(*tasks, return_exceptions=True)
            
            # Traite les r√©sultats
            for i, result in enumerate(search_results):
                if isinstance(result, Exception):
                    self.logger.warning(f"Erreur requ√™te {i+1}: {result}")
                    continue
                
                if result:
                    all_contents.extend(result)
                    self.logger.debug(f"‚úÖ {len(result)} papers de la requ√™te {i+1}")
        
        self.logger.info(f"üì• {len(all_contents)} papers collect√©s bruts")
        
        # Post-traitement
        if all_contents:
            # 1. Nettoie et valide
            all_contents = self.clean_and_validate(all_contents)
            
            # 2. Filtre par mots-cl√©s (filtrage plus fin)
            all_contents = self.filter_by_keywords(all_contents)
            
            # 3. D√©duplique (par ID ArXiv)
            all_contents = self._deduplicate(all_contents)
            
            # 4. Trie par date de soumission (plus r√©cent d'abord)
            all_contents.sort(
                key=lambda x: x.published_date or datetime.min.replace(tzinfo=timezone.utc), 
                reverse=True
            )
            
            # 5. Limite les r√©sultats
            all_contents = all_contents[:limit]
        
        self.logger.info(f"‚úÖ Collecte ArXiv termin√©e: {len(all_contents)} papers finaux")
        return all_contents
    
    def _build_search_queries(self) -> List[str]:
        """
        Construit les requ√™tes de recherche ArXiv.
        
        Combine les cat√©gories et mots-cl√©s pour cr√©er des requ√™tes optimales.
        
        Returns:
            Liste des requ√™tes de recherche √† ex√©cuter
        """
        queries = []
        
        # Date limite pour les papers r√©cents
        date_limit = datetime.now(timezone.utc) - timedelta(days=self.days_back)
        date_str = date_limit.strftime("%Y%m%d%H%M%S")
        
        # 1. Recherche par cat√©gories (papers r√©cents dans chaque cat√©gorie)
        for category in self.categories:
            query = f"cat:{category} AND submittedDate:[{date_str} TO *]"
            queries.append(query)
        
        # 2. Recherche par mots-cl√©s importants (dans titre et abstract)
        important_keywords = [
            "large language model",
            "transformer",
            "generative AI",
            "GPT",
            "BERT",
            "neural network",
            "deep learning"
        ]
        
        for keyword in important_keywords:
            # Recherche dans titre ET abstract, avec limite de date
            query = f"(ti:\"{keyword}\" OR abs:\"{keyword}\") AND submittedDate:[{date_str} TO *]"
            queries.append(query)
        
        self.logger.debug(f"Construites {len(queries)} requ√™tes de recherche")
        return queries
    
    async def _execute_search(self, session: aiohttp.ClientSession, query: str, max_results: int) -> List[RawContent]:
        """
        Ex√©cute une requ√™te de recherche ArXiv.
        
        Args:
            session: Session HTTP
            query: Requ√™te de recherche ArXiv
            max_results: Nombre maximum de r√©sultats
            
        Returns:
            Liste des papers trouv√©s
        """
        try:
            # Construction de l'URL de requ√™te
            params = {
                'search_query': query,
                'start': 0,
                'max_results': max_results,
                'sortBy': 'submittedDate',
                'sortOrder': 'descending'
            }
            
            # Encode les param√®tres
            param_string = '&'.join([f"{k}={quote_plus(str(v))}" for k, v in params.items()])
            url = f"{self.base_url}?{param_string}"
            
            self.logger.debug(f"üì° Requ√™te ArXiv: {query[:100]}...")
            
            async with session.get(url) as response:
                if response.status != 200:
                    self.logger.warning(f"HTTP {response.status} pour requ√™te ArXiv")
                    return []
                
                # R√©cup√®re le contenu XML
                xml_content = await response.text()
                
                # Parse la r√©ponse XML
                papers = self._parse_arxiv_response(xml_content)
                
                self.logger.debug(f"‚úÖ {len(papers)} papers pars√©s")
                return papers
        
        except asyncio.TimeoutError:
            self.logger.error(f"‚è∞ Timeout sur requ√™te ArXiv: {query[:50]}...")
            return []
        except Exception as e:
            self.logger.error(f"‚ùå Erreur requ√™te ArXiv: {e}")
            return []
    
    def _parse_arxiv_response(self, xml_content: str) -> List[RawContent]:
        """
        Parse la r√©ponse XML de l'API ArXiv.
        
        Args:
            xml_content: Contenu XML de la r√©ponse ArXiv
            
        Returns:
            Liste des papers pars√©s
        """
        papers = []
        
        try:
            # Parse le XML
            root = ET.fromstring(xml_content)
            
            # Namespace ArXiv
            namespaces = {
                'atom': 'http://www.w3.org/2005/Atom',
                'arxiv': 'http://arxiv.org/schemas/atom'
            }
            
            # Trouve tous les entries (papers)
            entries = root.findall('atom:entry', namespaces)
            
            for entry in entries:
                paper = self._parse_arxiv_entry(entry, namespaces)
                if paper:
                    papers.append(paper)
            
            self.logger.debug(f"Pars√©s {len(papers)} papers depuis XML ArXiv")
            
        except ET.ParseError as e:
            self.logger.error(f"Erreur parsing XML ArXiv: {e}")
        except Exception as e:
            self.logger.error(f"Erreur inattendue parsing ArXiv: {e}")
        
        return papers
    
    def _parse_arxiv_entry(self, entry, namespaces: Dict[str, str]) -> Optional[RawContent]:
        """
        Parse une entr√©e (paper) individuelle de la r√©ponse ArXiv.
        
        Args:
            entry: √âl√©ment XML de l'entr√©e
            namespaces: Namespaces XML
            
        Returns:
            RawContent ou None si invalide
        """
        try:
            # Titre
            title_elem = entry.find('atom:title', namespaces)
            if title_elem is None or not title_elem.text:
                return None
            title = title_elem.text.strip().replace('\n', ' ').replace('  ', ' ')
            
            # URL (lien vers la page ArXiv)
            link_elem = entry.find('atom:id', namespaces)
            if link_elem is None or not link_elem.text:
                return None
            arxiv_url = link_elem.text.strip()
            
            # Abstract (contenu principal)
            summary_elem = entry.find('atom:summary', namespaces)
            abstract = ""
            if summary_elem is not None and summary_elem.text:
                abstract = summary_elem.text.strip().replace('\n', ' ').replace('  ', ' ')
            
            # Auteurs
            authors = []
            author_elems = entry.findall('atom:author/atom:name', namespaces)
            for author_elem in author_elems:
                if author_elem.text:
                    authors.append(author_elem.text.strip())
            author_str = ", ".join(authors) if authors else ""
            
            # Date de publication/soumission
            published_elem = entry.find('atom:published', namespaces)
            published_date = None
            if published_elem is not None and published_elem.text:
                try:
                    # Format ArXiv: 2024-01-15T09:30:00Z
                    date_str = published_elem.text.strip()
                    published_date = datetime.fromisoformat(date_str.replace('Z', '+00:00'))
                except ValueError:
                    self.logger.warning(f"Format de date invalide: {published_elem.text}")
            
            # Cat√©gories ArXiv
            categories = []
            category_elems = entry.findall('atom:category', namespaces)
            for cat_elem in category_elems:
                term = cat_elem.get('term')
                if term:
                    categories.append(term)
            
            # DOI (si disponible)
            doi = ""
            doi_elem = entry.find('arxiv:doi', namespaces)
            if doi_elem is not None and doi_elem.text:
                doi = doi_elem.text.strip()
            
            # Commentaires (info additionnelle)
            comment = ""
            comment_elem = entry.find('arxiv:comment', namespaces)
            if comment_elem is not None and comment_elem.text:
                comment = comment_elem.text.strip()
            
            # Journal reference (si publi√©)
            journal_ref = ""
            journal_elem = entry.find('arxiv:journal_ref', namespaces)
            if journal_elem is not None and journal_elem.text:
                journal_ref = journal_elem.text.strip()
            
            # Extraction de l'ID ArXiv depuis l'URL
            arxiv_id = self._extract_arxiv_id(arxiv_url)
            
            # URL du PDF
            pdf_url = f"https://arxiv.org/pdf/{arxiv_id}.pdf" if arxiv_id else ""
            
            # Donn√©es brutes pour analyse future
            raw_data = {
                'arxiv_id': arxiv_id,
                'doi': doi,
                'comment': comment,
                'journal_ref': journal_ref,
                'pdf_url': pdf_url,
                'categories': categories,
                'authors_list': authors
            }
            
            return RawContent(
                title=title,
                url=arxiv_url,
                source="arxiv",
                content=abstract,  # L'abstract est le contenu principal
                excerpt=abstract[:500] + "..." if len(abstract) > 500 else abstract,
                author=author_str,
                published_date=published_date,
                tags=categories,  # Les cat√©gories ArXiv comme tags
                raw_data=raw_data
            )
            
        except Exception as e:
            self.logger.error(f"Erreur parsing entr√©e ArXiv: {e}")
            return None
    
    def _extract_arxiv_id(self, arxiv_url: str) -> str:
        """
        Extrait l'ID ArXiv depuis l'URL.
        
        Args:
            arxiv_url: URL ArXiv (ex: http://arxiv.org/abs/2401.12345v1)
            
        Returns:
            ID ArXiv (ex: 2401.12345v1)
        """
        try:
            # Format typique: http://arxiv.org/abs/ID
            if '/abs/' in arxiv_url:
                return arxiv_url.split('/abs/')[-1]
            return ""
        except:
            return ""
    
    def _deduplicate(self, contents: List[RawContent]) -> List[RawContent]:
        """
        Supprime les doublons bas√©s sur l'ID ArXiv.
        
        Args:
            contents: Liste avec potentiels doublons
            
        Returns:
            Liste sans doublons
        """
        seen_ids = set()
        unique_contents = []
        
        for content in contents:
            arxiv_id = content.raw_data.get('arxiv_id', '')
            # Utilise l'ID ArXiv ou l'URL comme cl√© de d√©duplication
            dedup_key = arxiv_id or content.url
            
            if dedup_key not in seen_ids:
                seen_ids.add(dedup_key)
                unique_contents.append(content)
            else:
                self.logger.debug(f"üîÑ Doublon ArXiv supprim√©: {content.title[:50]}...")
        
        self.logger.info(f"D√©duplication ArXiv: {len(contents)} ‚Üí {len(unique_contents)} papers")
        return unique_contents
    
    def is_available(self) -> bool:
        """
        V√©rifie si l'API ArXiv est accessible.
        
        Returns:
            True si l'API r√©pond correctement
        """
        import requests
        
        try:
            # Test avec une requ√™te simple
            test_url = f"{self.base_url}?search_query=cat:cs.AI&max_results=1"
            response = requests.get(
                test_url,
                timeout=10,
                headers={"User-Agent": self.user_agent}
            )
            
            if response.status_code == 200:
                # V√©rifie que c'est bien du XML ArXiv
                content = response.text
                return '<feed xmlns="http://www.w3.org/2005/Atom"' in content
            
            return False
            
        except Exception as e:
            self.logger.error(f"Test disponibilit√© ArXiv √©chou√©: {e}")
            return False
    
    async def test_connection(self) -> dict:
        """
        Teste la connexion ArXiv et retourne des statistiques.
        
        Returns:
            Dictionnaire avec les r√©sultats de test
        """
        self.logger.info("üß™ Test de connexion ArXiv...")
        
        results = {
            'available': False,
            'categories_tested': len(self.categories),
            'keywords_used': len(self.search_keywords),
            'sample_papers': [],
            'errors': []
        }
        
        try:
            # Test avec collecte limit√©e
            sample_papers = await self.collect(limit=3)
            
            results['available'] = len(sample_papers) > 0
            results['sample_papers'] = [
                {
                    'title': paper.title[:100],
                    'authors': paper.author[:100] if paper.author else "N/A",
                    'date': paper.published_date.isoformat() if paper.published_date else None,
                    'categories': paper.tags[:5],  # Limite les cat√©gories affich√©es
                    'arxiv_id': paper.raw_data.get('arxiv_id', ''),
                    'url': paper.url
                }
                for paper in sample_papers[:3]
            ]
            
        except Exception as e:
            results['errors'].append(str(e))
            self.logger.error(f"Test connexion ArXiv √©chou√©: {e}")
        
        self.logger.info(f"‚úÖ Test ArXiv termin√©: {len(results['sample_papers'])} papers trouv√©s")
        return results
    
    def get_paper_pdf_url(self, content: RawContent) -> str:
        """
        Retourne l'URL du PDF pour un paper ArXiv.
        
        Args:
            content: Contenu ArXiv
            
        Returns:
            URL du PDF ou cha√Æne vide
        """
        if content.source != "arxiv":
            return ""
        
        return content.raw_data.get('pdf_url', '')
    
    def get_categories_info(self) -> Dict[str, str]:
        """
        Retourne les informations sur les cat√©gories ArXiv surveill√©es.
        
        Returns:
            Dictionnaire {cat√©gorie: description}
        """
        category_descriptions = {
            "cs.AI": "Artificial Intelligence",
            "cs.CL": "Computation and Language (NLP)",
            "cs.CV": "Computer Vision and Pattern Recognition",
            "cs.HC": "Human-Computer Interaction",
            "cs.IR": "Information Retrieval",
            "cs.LG": "Machine Learning",
            "cs.MA": "Multiagent Systems",
            "cs.NE": "Neural and Evolutionary Computing",
            "stat.ML": "Machine Learning (Statistics)"
        }
        
        return {cat: category_descriptions.get(cat, "Unknown category") for cat in self.categories}
