"""
Point d'entrÃ©e principal de l'Agent de Veille Intelligente - Version ComplÃ¨te

Orchestrateur des 3 agents : Collecteur â†’ Analyseur â†’ SynthÃ©tiseur
GÃ©nÃ¨re automatiquement un digest quotidien de veille technologique.
"""
import asyncio
import sys
import os
from datetime import datetime
from loguru import logger

# Configuration du logging
logger.remove()
logger.add(
    sys.stdout, 
    format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> | {message}",
    level="INFO"
)

from src.utils.config import validate_config
from src.models.database import DatabaseManager
from src.agents import (
    TechCollectorAgent, CollectionConfig,
    TechAnalyzerAgent, TechSynthesizerAgent
)


async def create_daily_digest():
    """Workflow principal de crÃ©ation du digest quotidien."""
    
    logger.info("ğŸš€ DÃ‰MARRAGE AGENT DE VEILLE INTELLIGENTE")
    logger.info("ğŸ¯ GÃ©nÃ©ration du digest quotidien GenAI/LLM/Agentic")
    
    start_time = datetime.now()
    
    try:
        # ===============================
        # CONFIGURATION ET VALIDATION
        # ===============================
        logger.info("ğŸ”§ Validation de la configuration...")
        validate_config()
        
        logger.info("ğŸ’¾ Initialisation base de donnÃ©es...")
        db = DatabaseManager()
        
        # Configuration pour la collecte production
        collection_config = CollectionConfig(
            total_limit=15,  # Nombre optimisÃ© pour analyse qualitative
            source_limits={'medium': 8, 'arxiv': 8},
            keywords=[
                'AI', 'GenAI', 'LLM', 'GPT', 'ChatGPT',
                'LangChain', 'LangGraph', 'transformer',
                'machine learning', 'deep learning',
                'neural network', 'agentic', 'multi-agent',
                'artificial intelligence'
            ],
            max_age_days=30,  # Articles rÃ©cents mais suffisamment de contenu
            enable_deduplication=True
        )
        
        logger.info("âœ… Configuration validÃ©e")
        
        # ===============================
        # PHASE 1: COLLECTE
        # ===============================
        logger.info("\nğŸ“¡ PHASE 1: Collecte de contenu...")
        
        collector = TechCollectorAgent(collection_config)
        collection_result = await collector.collect_all_sources()
        
        if collection_result.total_filtered == 0:
            logger.error("âŒ Aucun contenu collectÃ© - ArrÃªt du processus")
            return None
        
        logger.info(f"âœ… Collecte rÃ©ussie:")
        logger.info(f"   ğŸ“Š {collection_result.total_collected} articles rÃ©cupÃ©rÃ©s")
        logger.info(f"   âœ… {collection_result.total_filtered} articles filtrÃ©s")
        logger.info(f"   ğŸ”„ {collection_result.duplicates_removed} doublons supprimÃ©s")
        logger.info(f"   â±ï¸ {collection_result.collection_time:.2f}s")
        
        # ===============================
        # PHASE 2: ANALYSE
        # ===============================
        logger.info("\nğŸ§  PHASE 2: Analyse intelligente...")
        
        analyzer = TechAnalyzerAgent()
        analyzed_articles = await analyzer.analyze_contents(collection_result.contents)
        
        if not analyzed_articles:
            logger.error("âŒ Aucun article analysÃ© - ArrÃªt du processus")
            return None
        
        recommended_articles = [a for a in analyzed_articles if a.analysis.recommended]
        avg_score = sum(a.final_score for a in analyzed_articles) / len(analyzed_articles)
        
        logger.info(f"âœ… Analyse rÃ©ussie:")
        logger.info(f"   ğŸ“Š {len(analyzed_articles)} articles analysÃ©s")
        logger.info(f"   ğŸ¯ {len(recommended_articles)} articles recommandÃ©s")
        logger.info(f"   ğŸ“ˆ Score moyen: {avg_score:.2f}/1.0")
        
        # ===============================
        # PHASE 3: SYNTHÃˆSE
        # ===============================
        logger.info("\nğŸ“ PHASE 3: GÃ©nÃ©ration du digest...")
        
        synthesizer = TechSynthesizerAgent()
        daily_digest = await synthesizer.create_daily_digest(analyzed_articles)
        
        # Sauvegarde du digest
        output_path = await synthesizer.save_digest_to_file(daily_digest)
        
        logger.info(f"âœ… Digest gÃ©nÃ©rÃ©:")
        logger.info(f"   ğŸ“‹ {daily_digest.title}")
        logger.info(f"   ğŸ“„ {daily_digest.word_count} mots ({daily_digest.estimated_read_time}min)")
        logger.info(f"   ğŸ† {len(daily_digest.top_articles)} articles vedettes")
        logger.info(f"   ğŸ’¡ {len(daily_digest.key_insights)} insights clÃ©s")
        logger.info(f"   ğŸ¯ {len(daily_digest.recommendations)} recommandations")
        logger.info(f"   ğŸ’¾ SauvegardÃ©: {output_path}")
        
        # ===============================
        # RÃ‰SUMÃ‰ FINAL
        # ===============================
        total_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"\nğŸ‰ DIGEST QUOTIDIEN GÃ‰NÃ‰RÃ‰ AVEC SUCCÃˆS!")
        logger.info(f"â±ï¸ Temps total: {total_time:.2f}s")
        logger.info(f"ğŸ“Š Performance: {total_time/len(analyzed_articles):.2f}s/article")
        logger.info(f"ğŸ“„ Fichier: {output_path}")
        
        # AperÃ§u du contenu
        logger.info(f"\nğŸ“‹ APERÃ‡U DU DIGEST:")
        logger.info(f"ğŸ—“ï¸ {daily_digest.title}")
        
        # Top articles
        for i, article in enumerate(daily_digest.top_articles[:2], 1):
            logger.info(f"   {i}. {article.title_refined}")
            logger.info(f"      ğŸ“Š Score: {article.relevance_for_audience:.2f} | {article.complexity_level}")
        
        # Top insights
        for insight in daily_digest.key_insights[:2]:
            logger.info(f"   ğŸ’¡ {insight}")
        
        return {
            'digest': daily_digest,
            'output_path': output_path,
            'stats': {
                'collection': collection_result,
                'analysis': analyzed_articles,
                'total_time': total_time
            }
        }
        
    except Exception as e:
        logger.error(f"âŒ Erreur lors de l'exÃ©cution: {e}")
        logger.exception("DÃ©tails de l'erreur:")
        raise


async def run_quick_demo():
    """DÃ©mo rapide avec moins d'articles pour test."""
    
    logger.info("ğŸš€ MODE DÃ‰MO RAPIDE")
    
    # Configuration allÃ©gÃ©e
    demo_config = CollectionConfig(
        total_limit=6,
        source_limits={'medium': 3, 'arxiv': 3},
        keywords=['AI', 'LLM', 'machine learning'],
        max_age_days=60
    )
    
    try:
        # Collecte
        collector = TechCollectorAgent(demo_config)
        collection_result = await collector.collect_all_sources()
        
        # Analyse
        analyzer = TechAnalyzerAgent()
        analyzed_articles = await analyzer.analyze_contents(collection_result.contents)
        
        # SynthÃ¨se
        synthesizer = TechSynthesizerAgent()
        daily_digest = await synthesizer.create_daily_digest(analyzed_articles)
        
        # Sauvegarde
        output_path = await synthesizer.save_digest_to_file(daily_digest)
        
        logger.info(f"âœ… DÃ©mo terminÃ©e - Digest: {output_path}")
        return daily_digest
        
    except Exception as e:
        logger.error(f"âŒ Erreur dÃ©mo: {e}")
        raise


def main():
    """Fonction principale avec gestion des modes."""
    
    # DÃ©tection du mode
    demo_mode = "--demo" in sys.argv or "-d" in sys.argv
    
    if demo_mode:
        logger.info("ğŸ¬ Lancement en mode DÃ‰MO (collecte rÃ©duite)")
        result = asyncio.run(run_quick_demo())
    else:
        logger.info("ğŸ¯ Lancement en mode PRODUCTION (collecte complÃ¨te)")
        result = asyncio.run(create_daily_digest())
    
    if result:
        logger.info("ğŸ‰ Traitement terminÃ© avec succÃ¨s!")
    else:
        logger.error("âŒ Ã‰chec du traitement")
        sys.exit(1)


if __name__ == "__main__":
    main()
