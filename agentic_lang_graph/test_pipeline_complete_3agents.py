"""
Test UAT Pipeline Complet - 3 Agents
Collecteur â†’ Analyseur â†’ SynthÃ©tiseur avec vraies donnÃ©es live
"""
import asyncio
import sys
import os
from datetime import datetime
from loguru import logger

# Configuration du logging pour le test
logger.remove()
logger.add(sys.stdout, format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | {message}")

# Ajout du chemin pour les imports
sys.path.append(os.path.dirname(os.path.abspath(__file__)))

from src.agents.tech_collector_agent import TechCollectorAgent, CollectionConfig
from src.agents.tech_analyzer_agent import TechAnalyzerAgent
from src.agents.tech_synthesizer_agent import TechSynthesizerAgent


async def test_complete_pipeline():
    """Test UAT complet du pipeline 3 agents avec vraies donnÃ©es."""
    
    logger.info("ğŸš€ DÃ‰BUT TEST UAT PIPELINE COMPLET - 3 AGENTS")
    logger.info("=" * 70)
    
    start_time = datetime.now()
    
    try:
        # ===============================
        # PHASE 1: AGENT COLLECTEUR
        # ===============================
        logger.info("ğŸ“¡ PHASE 1: Agent Collecteur Tech")
        
        # Configuration optimisÃ©e pour test rapide mais reprÃ©sentatif
        collection_config = CollectionConfig(
            total_limit=10,  # Plus d'articles pour test synthÃ¨se
            source_limits={'medium': 6, 'arxiv': 6},
            keywords=['AI', 'LLM', 'machine learning', 'GenAI', 'transformer', 'agent'],
            max_age_days=60,  # Plus large pour avoir assez de contenu
            enable_deduplication=True
        )
        
        logger.info(f"ğŸ”§ Configuration: {collection_config.total_limit} articles max")
        logger.info(f"ğŸ“š Mots-clÃ©s: {collection_config.keywords[:3]}...")
        
        # Collecte
        collector = TechCollectorAgent(collection_config)
        phase1_start = datetime.now()
        collection_result = await collector.collect_all_sources()
        phase1_time = (datetime.now() - phase1_start).total_seconds()
        
        # Validation Phase 1
        assert collection_result.total_filtered > 0, "âŒ Aucun contenu collectÃ©"
        assert len(collection_result.contents) > 0, "âŒ Liste de contenus vide"
        
        logger.info("âœ… PHASE 1 RÃ‰USSIE")
        logger.info(f"   ğŸ“Š CollectÃ©s: {collection_result.total_collected}")
        logger.info(f"   âœ… FiltrÃ©s: {collection_result.total_filtered}")
        logger.info(f"   ğŸ”„ Doublons: {collection_result.duplicates_removed}")
        logger.info(f"   â±ï¸ Temps: {phase1_time:.2f}s")
        
        # Ã‰chantillon Phase 1
        logger.info("\nğŸ“„ Ã‰CHANTILLON COLLECTÃ‰:")
        for i, content in enumerate(collection_result.contents[:3], 1):
            logger.info(f"   {i}. {content.title[:60]}...")
            logger.info(f"      ğŸ“ {content.source} | {content.published_date}")
        
        # ===============================
        # PHASE 2: AGENT ANALYSEUR
        # ===============================
        logger.info("\nğŸ§  PHASE 2: Agent Analyseur avec LangGraph")
        
        analyzer = TechAnalyzerAgent()
        logger.info("ğŸ”§ Agent Analyseur initialisÃ©")
        
        phase2_start = datetime.now()
        analyzed_articles = await analyzer.analyze_contents(collection_result.contents)
        phase2_time = (datetime.now() - phase2_start).total_seconds()
        
        # Validation Phase 2
        assert len(analyzed_articles) >= 0, "âŒ Erreur dans l'analyse"
        
        logger.info("âœ… PHASE 2 RÃ‰USSIE")
        logger.info(f"   ğŸ“Š Articles analysÃ©s: {len(analyzed_articles)}")
        logger.info(f"   â±ï¸ Temps: {phase2_time:.2f}s")
        
        if analyzed_articles:
            recommended_count = sum(1 for a in analyzed_articles if a.analysis.recommended)
            avg_score = sum(a.final_score for a in analyzed_articles) / len(analyzed_articles)
            
            logger.info(f"   ğŸ¯ RecommandÃ©s: {recommended_count}/{len(analyzed_articles)}")
            logger.info(f"   ğŸ“ˆ Score moyen: {avg_score:.2f}")
            
            # Top articles de Phase 2
            logger.info("\nğŸ† TOP ARTICLES ANALYSÃ‰S:")
            for i, article in enumerate(analyzed_articles[:3], 1):
                status = "âœ…" if article.analysis.recommended else "âŒ"
                logger.info(f"   {i}. {status} Score: {article.final_score:.2f}/1.0")
                logger.info(f"      ğŸ“„ {article.raw_content.title[:50]}...")
                logger.info(f"      ğŸ” {article.analysis.category} | {article.analysis.expertise_level}")
        
        # ===============================
        # PHASE 3: AGENT SYNTHÃ‰TISEUR
        # ===============================
        logger.info("\nğŸ“ PHASE 3: Agent SynthÃ©tiseur - CrÃ©ation Digest")
        
        synthesizer = TechSynthesizerAgent()
        logger.info("ğŸ”§ Agent SynthÃ©tiseur initialisÃ©")
        
        # Configuration spÃ©cifique pour test
        synthesizer.config.update({
            "max_articles_in_digest": 3,
            "max_insights": 4,
            "max_recommendations": 3
        })
        
        phase3_start = datetime.now()
        daily_digest = await synthesizer.create_daily_digest(analyzed_articles)
        phase3_time = (datetime.now() - phase3_start).total_seconds()
        
        # Validation Phase 3
        assert daily_digest is not None, "âŒ Ã‰chec gÃ©nÃ©ration digest"
        assert daily_digest.markdown_content is not None, "âŒ Contenu Markdown manquant"
        assert len(daily_digest.top_articles) > 0, "âŒ Aucun article dans le digest"
        
        logger.info("âœ… PHASE 3 RÃ‰USSIE")
        logger.info(f"   ğŸ“ Digest crÃ©Ã©: {daily_digest.title}")
        logger.info(f"   ğŸ“Š Articles digest: {len(daily_digest.top_articles)}")
        logger.info(f"   ğŸ’¡ Insights: {len(daily_digest.key_insights)}")
        logger.info(f"   ğŸ¯ Recommandations: {len(daily_digest.recommendations)}")
        logger.info(f"   ğŸ“„ Mots: {daily_digest.word_count}")
        logger.info(f"   â±ï¸ Temps: {phase3_time:.2f}s")
        
        # Sauvegarde du digest
        output_path = await synthesizer.save_digest_to_file(daily_digest)
        logger.info(f"   ğŸ’¾ SauvegardÃ©: {output_path}")
        
        # ===============================
        # VALIDATION PIPELINE COMPLET
        # ===============================
        total_time = (datetime.now() - start_time).total_seconds()
        
        logger.info("\nğŸ¯ RÃ‰SULTATS PIPELINE COMPLET - 3 AGENTS")
        logger.info("=" * 60)
        logger.info(f"âœ… Phase 1 (Collecte): {collection_result.total_filtered} articles â†’ {phase1_time:.1f}s")
        logger.info(f"âœ… Phase 2 (Analyse): {len(analyzed_articles)} articles â†’ {phase2_time:.1f}s") 
        logger.info(f"âœ… Phase 3 (SynthÃ¨se): 1 digest â†’ {phase3_time:.1f}s")
        logger.info(f"â±ï¸ Temps total: {total_time:.2f}s")
        logger.info(f"ğŸ“Š Performance: {total_time/max(1, len(analyzed_articles)):.2f}s/article")
        
        # ===============================
        # APERÃ‡U DU DIGEST GÃ‰NÃ‰RÃ‰
        # ===============================
        logger.info("\nğŸ“‹ APERÃ‡U DU DIGEST GÃ‰NÃ‰RÃ‰:")
        logger.info(f"ğŸ—“ï¸ {daily_digest.title}")
        logger.info(f"ğŸ“ {daily_digest.subtitle}")
        
        # RÃ©sumÃ© exÃ©cutif (tronquÃ©)
        executive_preview = daily_digest.executive_summary[:150] + "..." if len(daily_digest.executive_summary) > 150 else daily_digest.executive_summary
        logger.info(f"\nğŸ“Š RÃ©sumÃ© ExÃ©cutif:\n{executive_preview}")
        
        # Top articles du digest
        logger.info(f"\nğŸ† Top {len(daily_digest.top_articles)} Articles:")
        for i, article in enumerate(daily_digest.top_articles, 1):
            logger.info(f"   {i}. {article.title_refined}")
            logger.info(f"      ğŸ“Š Score: {article.relevance_for_audience:.2f} | {article.complexity_level}")
            logger.info(f"      ğŸ’¡ {article.executive_summary[:80]}...")
        
        # Insights clÃ©s
        if daily_digest.key_insights:
            logger.info(f"\nğŸ’¡ Insights ClÃ©s:")
            for insight in daily_digest.key_insights[:3]:
                logger.info(f"   â€¢ {insight}")
        
        # Recommandations
        if daily_digest.recommendations:
            logger.info(f"\nğŸ¯ Recommandations:")
            for rec in daily_digest.recommendations[:2]:
                logger.info(f"   â€¢ {rec.title} ({rec.priority} priority)")
                logger.info(f"     {rec.description[:60]}...")
        
        # ===============================
        # MÃ‰TRIQUES QUALITÃ‰
        # ===============================
        logger.info("\nğŸ“ˆ MÃ‰TRIQUES DE QUALITÃ‰:")
        
        # Taux de conversion par phase
        collection_to_analysis = (len(analyzed_articles) / collection_result.total_filtered) * 100
        analysis_to_digest = (len(daily_digest.top_articles) / len(analyzed_articles)) * 100 if analyzed_articles else 0
        overall_conversion = (len(daily_digest.top_articles) / collection_result.total_filtered) * 100
        
        logger.info(f"   ğŸ“Š Collecte â†’ Analyse: {collection_to_analysis:.1f}%")
        logger.info(f"   ğŸ“Š Analyse â†’ Digest: {analysis_to_digest:.1f}%") 
        logger.info(f"   ğŸ“Š Conversion globale: {overall_conversion:.1f}%")
        logger.info(f"   ğŸ“Š Score qualitÃ© moyen: {daily_digest.average_quality_score:.2f}/1.0")
        
        # RÃ©partition par source
        sources_in_digest = {}
        for article in daily_digest.top_articles:
            source = article.original_article.raw_content.source
            sources_in_digest[source] = sources_in_digest.get(source, 0) + 1
        logger.info(f"   ğŸ“ Sources digest: {dict(sources_in_digest)}")
        
        # ===============================
        # VALIDATION INTÃ‰GRITÃ‰
        # ===============================
        logger.info("\nğŸ” VALIDATION INTÃ‰GRITÃ‰:")
        
        # VÃ©rifications de cohÃ©rence
        integrity_checks = []
        
        # CohÃ©rence des donnÃ©es
        if all(article.original_article is not None for article in daily_digest.top_articles):
            integrity_checks.append("âœ… Liens articles originaux prÃ©servÃ©s")
        else:
            integrity_checks.append("âŒ Liens articles originaux cassÃ©s")
        
        # QualitÃ© du contenu
        if daily_digest.word_count > 500:
            integrity_checks.append("âœ… Digest suffisamment dÃ©taillÃ©")
        else:
            integrity_checks.append("âš ï¸ Digest potentiellement trop court")
        
        # CohÃ©rence scores
        if all(0 <= article.relevance_for_audience <= 1 for article in daily_digest.top_articles):
            integrity_checks.append("âœ… Scores de relevance cohÃ©rents")
        else:
            integrity_checks.append("âŒ Scores de relevance incohÃ©rents")
        
        # Format Markdown
        if "# " in daily_digest.markdown_content and "## " in daily_digest.markdown_content:
            integrity_checks.append("âœ… Structure Markdown valide")
        else:
            integrity_checks.append("âŒ Structure Markdown invalide")
        
        for check in integrity_checks:
            logger.info(f"   {check}")
        
        # ===============================
        # SUCCÃˆS FINAL
        # ===============================
        logger.info("\nğŸ‰ TEST UAT PIPELINE COMPLET: âœ… SUCCÃˆS TOTAL")
        logger.info("=" * 70)
        logger.info("ğŸ† SYSTÃˆME OPÃ‰RATIONNEL - 3 AGENTS INTÃ‰GRÃ‰S")
        logger.info(f"ğŸ“„ Digest final sauvegardÃ©: {output_path}")
        
        return {
            'collection_result': collection_result,
            'analyzed_articles': analyzed_articles,
            'daily_digest': daily_digest,
            'total_time': total_time,
            'phases_time': {
                'collection': phase1_time,
                'analysis': phase2_time,
                'synthesis': phase3_time
            },
            'output_file': output_path,
            'success': True
        }
        
    except Exception as e:
        logger.error(f"âŒ Ã‰CHEC TEST UAT PIPELINE: {e}")
        logger.exception("DÃ©tails de l'erreur:")
        return {
            'error': str(e),
            'success': False
        }


async def demo_digest_preview():
    """DÃ©monstrateur rapide du format digest."""
    
    logger.info("\n" + "="*50)
    logger.info("ğŸ“‹ APERÃ‡U FORMAT DIGEST MARKDOWN")
    logger.info("="*50)
    
    # Simulation d'un digest exemple
    sample_content = """
# Tech Digest - 01 Juin 2025

> Veille technologique GenAI/LLM/Agentic pour ingÃ©nieurs seniors  
> ğŸ“… 01 Juin 2025 â€¢ ğŸ¯ senior_engineer â€¢ â±ï¸ 8 min de lecture

---

## ğŸ“Š RÃ©sumÃ© ExÃ©cutif

Les dÃ©veloppements d'aujourd'hui rÃ©vÃ¨lent une accÃ©lÃ©ration notable dans l'adoption des architectures multi-agents en production. Les Ã©quipes techniques se focalisent dÃ©sormais sur l'optimisation des performances LLM plutÃ´t que sur l'augmentation brute des capacitÃ©s. L'Ã©mergence de patterns standardisÃ©s pour LangGraph facilite l'implÃ©mentation d'agents complexes, tandis que les nouveaux frameworks d'orchestration promettent une meilleure observabilitÃ© des workflows IA.

**ğŸ“ˆ MÃ©triques de veille:**
- ğŸ“„ **Articles analysÃ©s:** 10
- âœ… **Articles recommandÃ©s:** 3
- ğŸ¯ **Score moyen de qualitÃ©:** 0.82/1.0

---

## ğŸ† Top Articles

### 1. ğŸš€ Production-Ready Multi-Agent Architectures with LangGraph

**ğŸ§  Advanced â€¢ â±ï¸ 12min â€¢ ğŸ“Š 0.89/1.0**

Cette analyse approfondie prÃ©sente des patterns d'architecture Ã©prouvÃ©s pour dÃ©ployer des systÃ¨mes multi-agents en production, avec un focus sur la gestion d'Ã©tat et l'observabilitÃ©.

**ğŸ”‘ Points clÃ©s:**
- Patterns de workflow orchestrÃ©s pour agents complexes
- StratÃ©gies de gestion d'Ã©tat centralisÃ©e avec StateGraph
- MÃ©triques et monitoring pour systÃ¨mes agentic en production

**âš™ï¸ Aspects techniques:**
- ImplÃ©mentation de checkpoints pour rÃ©cupÃ©ration d'erreurs
- Optimisation mÃ©moire pour workflows long-running
- IntÃ©gration avec systÃ¨mes de monitoring existants

ğŸ”— **Source:** [arxiv](https://arxiv.org/abs/...)

---

## ğŸ’¡ Insights ClÃ©s

- **L'orchestration multi-agents devient mainstream avec LangGraph comme standard de facto**
- **Les Ã©quipes privilÃ©gient l'efficacitÃ© opÃ©rationnelle aux performances brutes des LLM**
- **L'observabilitÃ© Ã©merge comme dÃ©fi principal des systÃ¨mes IA en production**

---

## ğŸ¯ Recommandations Actionables

### 1. ğŸ”¥ Ã‰valuer LangGraph pour vos workflows IA

**ğŸ“š Learning â€¢ â±ï¸ 1-4h â€¢ ğŸ¯ High priority**

Avec la standardisation croissante de LangGraph, Ã©valuer son adoption pour vos systÃ¨mes multi-agents actuels pourrait simplifier l'architecture et amÃ©liorer la maintenabilitÃ©.

**Actions concrÃ¨tes:**
- [ ] Analyser vos workflows IA existants
- [ ] CrÃ©er un POC avec LangGraph
- [ ] Comparer avec votre solution actuelle

---

*Digest gÃ©nÃ©rÃ© le 01/06/2025 Ã  14:30 par 1.0 â€¢ LLM: gpt-4o*
"""
    
    logger.info(sample_content)
    logger.info("\nâœ… Le pipeline gÃ©nÃ¨re des digests dans ce format")


if __name__ == "__main__":
    # ExÃ©cution du test UAT complet
    print("ğŸš€ Lancement du test UAT pipeline complet (3 agents)...")
    print("âš ï¸ Ce test utilise de vraies donnÃ©es et l'API OpenAI")
    print()
    
    result = asyncio.run(test_complete_pipeline())
    
    if result['success']:
        print("\n" + "="*70)
        print("ğŸ‰ TEST UAT RÃ‰USSI - PIPELINE 3 AGENTS OPÃ‰RATIONNEL")
        print("="*70)
        print(f"ğŸ“„ Digest sauvegardÃ©: {result['output_file']}")
        print("ğŸš€ SystÃ¨me prÃªt pour production")
        
        # AperÃ§u du format
        asyncio.run(demo_digest_preview())
        
    else:
        print("\n" + "="*70)
        print("âŒ TEST UAT Ã‰CHOUÃ‰")
        print("="*70)
        sys.exit(1)
