"""
Point d'entr√©e principal enrichi de l'Agent de Veille Intelligente - Phase 3

Version enrichie avec int√©gration base de donn√©es :
- D√©duplication intelligente des articles
- Cache des analyses pour optimiser les performances  
- Historique complet et m√©triques avanc√©es
- Suivi des performances en temps r√©el
"""
import asyncio
import sys
import os
import argparse
from datetime import datetime
from loguru import logger

# Imports de la configuration centralis√©e
from src.utils.config_loader import load_config
from src.models.database_enhanced import DatabaseManagerEnhanced
from src.services.veille_integration_service import VeilleIntegrationService
from src.agents import (
    TechCollectorAgent, CollectionConfig,
    TechAnalyzerAgent, TechSynthesizerAgent
)


def setup_logging(level: str = "INFO"):
    """Configure le logging."""
    logger.remove()
    logger.add(
        sys.stdout, 
        format="<green>{time:HH:mm:ss}</green> | <level>{level: <8}</level> | <cyan>{function}</cyan> | {message}",
        level=level
    )


def parse_arguments():
    """Parse les arguments de ligne de commande avec nouvelles options Phase 3."""
    parser = argparse.ArgumentParser(
        description="Agent de Veille Intelligente ENRICHI - G√©n√©rateur de digest avec BD",
        formatter_class=argparse.RawDescriptionHelpFormatter,
        epilog="""
Exemples d'utilisation Phase 3:
  python main_enhanced.py                          # Mode production avec BD enrichie
  python main_enhanced.py --demo                   # Mode d√©mo avec BD
  python main_enhanced.py --profile expert         # Profil expert avec cache
  python main_enhanced.py --skip-cache             # Forcer analyse (ignorer cache)
  python main_enhanced.py --cleanup-old            # Nettoyer anciennes donn√©es
  python main_enhanced.py --show-stats             # Afficher statistiques BD
  python main_enhanced.py --cache-max-age 12       # Cache max 12h
        """
    )
    
    # Modes de fonctionnement
    parser.add_argument(
        "--demo", "-d", 
        action="store_true", 
        help="Mode d√©mo avec collecte r√©duite"
    )
    
    # Configuration
    parser.add_argument(
        "--profile", "-p", 
        choices=["demo", "production", "expert"],
        help="Profil de configuration √† utiliser"
    )
    
    parser.add_argument(
        "--environment", "-e", 
        choices=["development", "production"],
        help="Environnement d'ex√©cution"
    )
    
    # Options Phase 3 - Base de donn√©es
    parser.add_argument(
        "--skip-cache", 
        action="store_true", 
        help="Ignorer le cache et forcer l'analyse de tous les articles"
    )
    
    parser.add_argument(
        "--cache-max-age", 
        type=int, 
        default=24,
        help="√Çge maximum du cache en heures (d√©faut: 24h)"
    )
    
    parser.add_argument(
        "--cleanup-old", 
        action="store_true", 
        help="Nettoyer les anciennes donn√©es avant ex√©cution"
    )
    
    parser.add_argument(
        "--show-stats", 
        action="store_true", 
        help="Afficher les statistiques d√©taill√©es de la BD"
    )
    
    parser.add_argument(
        "--db-path", 
        help="Chemin personnalis√© vers la base de donn√©es"
    )
    
    # Overrides de configuration
    parser.add_argument(
        "--total-limit", 
        type=int, 
        help="Nombre maximum d'articles √† collecter"
    )
    
    parser.add_argument(
        "--target-audience", 
        choices=["senior_engineer", "tech_lead", "architect"],
        help="Audience cible pour le digest"
    )
    
    parser.add_argument(
        "--max-articles", 
        type=int, 
        help="Nombre maximum d'articles dans le digest final"
    )
    
    # Options de sortie
    parser.add_argument(
        "--output-dir", 
        help="R√©pertoire de sortie pour les digests"
    )
    
    # Logging
    parser.add_argument(
        "--log-level", 
        choices=["DEBUG", "INFO", "WARNING", "ERROR"],
        default="INFO",
        help="Niveau de logging"
    )
    
    parser.add_argument(
        "--verbose", "-v", 
        action="store_true", 
        help="Mode verbose (equivalent √† --log-level DEBUG)"
    )
    
    return parser.parse_args()


async def create_daily_digest_enhanced(profile: str = None, 
                                     environment: str = None, 
                                     db_path: str = None,
                                     cache_max_age_hours: int = 24,
                                     skip_cache: bool = False,
                                     **overrides):
    """Workflow principal enrichi de cr√©ation du digest quotidien avec BD."""
    
    logger.info("üöÄ D√âMARRAGE AGENT DE VEILLE INTELLIGENTE ENRICHI")
    logger.info("üóÑÔ∏è Version Phase 3 avec Base de Donn√©es Avanc√©e")
    logger.info("üéØ G√©n√©ration du digest quotidien GenAI/LLM/Agentic")
    
    start_time = datetime.now()
    
    try:
        # ===============================
        # CONFIGURATION ET VALIDATION
        # ===============================
        logger.info("üîß Chargement de la configuration...")
        config = load_config(profile=profile, environment=environment)
        
        # Application des overrides CLI
        if overrides:
            logger.info(f"üîß Application overrides: {overrides}")
            if 'total_limit' in overrides:
                config.collection.total_limit = overrides['total_limit']
            if 'target_audience' in overrides:
                config.synthesis.target_audience = overrides['target_audience']
            if 'max_articles' in overrides:
                config.synthesis.max_articles_in_digest = overrides['max_articles']
        
        logger.info(f"‚öôÔ∏è Configuration:")
        logger.info(f"   üì° Collecte: {config.collection.total_limit} articles max")
        logger.info(f"   üéØ Audience: {config.synthesis.target_audience}")
        logger.info(f"   üìù Digest: {config.synthesis.max_articles_in_digest} articles")
        logger.info(f"   üíæ Cache: {'D√âSACTIV√â' if skip_cache else f'{cache_max_age_hours}h max'}")
        
        # ===============================
        # INITIALISATION SERVICE BD ENRICHIE
        # ===============================
        logger.info("üóÑÔ∏è Initialisation service BD enrichie...")
        integration_service = VeilleIntegrationService(db_path)
        
        # Configuration pour la collecte bas√©e sur le config file
        collection_config = CollectionConfig(
            total_limit=config.collection.total_limit,
            source_limits=config.collection.source_limits,
            keywords=config.collection.keywords,
            max_age_days=config.collection.max_age_days,
            enable_deduplication=True
        )
        
        logger.info("‚úÖ Configuration et BD enrichie valid√©es")
        
        # ===============================
        # PHASE 1: COLLECTE AVEC D√âDUPLICATION
        # ===============================
        logger.info("\nüì° PHASE 1: Collecte avec d√©duplication intelligente...")
        
        collector = TechCollectorAgent(collection_config)
        collection_result = await collector.collect_all_sources()
        
        if collection_result.total_filtered == 0:
            logger.error("‚ùå Aucun contenu collect√© - Arr√™t du processus")
            return None
        
        # Traitement avec d√©duplication via service enrichi
        dedup_result = await integration_service.process_collection_with_deduplication(collection_result)
        
        unique_contents = dedup_result['unique_contents']
        dedup_stats = dedup_result['deduplication_stats']
        
        if not unique_contents:
            logger.warning("‚ö†Ô∏è Tous les articles sont des doublons - Aucun nouveau contenu")
            # Afficher les statistiques quand m√™me
            integration_service.print_daily_summary()
            return None
        
        logger.info(f"‚úÖ Collecte avec d√©duplication r√©ussie:")
        logger.info(f"   üìä {dedup_stats['total_collected']} articles r√©cup√©r√©s")
        logger.info(f"   üÜï {dedup_stats['unique_articles']} articles uniques")
        logger.info(f"   üîÑ {dedup_stats['duplicates_removed']} doublons √©vit√©s")
        if dedup_stats['duplicates_by_type']:
            logger.info(f"   üìã Types de doublons: {dedup_stats['duplicates_by_type']}")
        
        # ===============================
        # PHASE 2: ANALYSE AVEC CACHE
        # ===============================
        logger.info("\nüß† PHASE 2: Analyse intelligente avec cache...")
        
        # Cr√©ation de l'expert profile depuis la config
        from src.models.analysis_models import ExpertProfile, ExpertLevel
        expert_profile = ExpertProfile(
            level=ExpertLevel(config.analysis.expert_level),
            interests=config.analysis.interests,
            avoid_topics=config.analysis.avoid_topics,
            preferred_content_types=config.analysis.preferred_content_types
        )
        
        analyzer = TechAnalyzerAgent(expert_profile)
        
        # Fonction d'analyse pour le service d'int√©gration
        async def analyze_single_content(raw_content):
            # Utiliser analyze_contents qui prend une liste et retourner le premier √©l√©ment
            results = await analyzer.analyze_contents([raw_content])
            if not results:
                raise ValueError(f"Aucune analyse produite pour: {raw_content.title}")
            return results[0]
        
        # Traitement avec cache via service enrichi
        if skip_cache:
            logger.info("‚ö†Ô∏è Cache d√©sactiv√© - Analyse forc√©e de tous les articles")
            analyzed_articles = await analyzer.analyze_contents(unique_contents)
            # Sauvegarde en cache pour les prochaines fois
            for analyzed_content in analyzed_articles:
                integration_service.db.save_analyzed_content(analyzed_content)
        else:
            analyzed_articles = await integration_service.process_analysis_with_cache(
                unique_contents, 
                analyze_single_content,
                cache_max_age_hours
            )
        
        if not analyzed_articles:
            logger.error("‚ùå Aucun article analys√© - Arr√™t du processus")
            return None
        
        recommended_articles = [a for a in analyzed_articles if a.analysis.recommended]
        avg_score = sum(a.analysis.relevance_score for a in analyzed_articles) / len(analyzed_articles)
        
        logger.info(f"‚úÖ Analyse avec cache r√©ussie:")
        logger.info(f"   üìä {len(analyzed_articles)} articles analys√©s")
        logger.info(f"   üéØ {len(recommended_articles)} articles recommand√©s")
        logger.info(f"   üìà Score moyen: {avg_score:.2f}/10.0")
        
        # ===============================
        # PHASE 3: SYNTH√àSE AVEC HISTORIQUE
        # ===============================
        logger.info("\nüìù PHASE 3: G√©n√©ration du digest avec historique...")
        
        # Configuration du synth√©tiseur avec la config centralis√©e
        synthesis_config = {
            "target_audience": config.synthesis.target_audience,
            "max_articles_in_digest": config.synthesis.max_articles_in_digest,
            "executive_summary_max_words": config.synthesis.executive_summary_max_words,
            "article_summary_max_words": config.synthesis.article_summary_max_words,
            "max_insights": config.synthesis.max_insights,
            "max_recommendations": config.synthesis.max_recommendations,
            "include_technical_trends": config.synthesis.include_technical_trends,
            "include_action_items": config.synthesis.include_action_items,
            "tone": config.synthesis.tone,
            "technical_depth": config.synthesis.technical_depth,
            "focus_areas": config.synthesis.focus_areas
        }
        
        synthesizer = TechSynthesizerAgent(synthesis_config)
        
        # Fonction de synth√®se pour le service d'int√©gration
        async def synthesize_content(analyzed_contents):
            return await synthesizer.create_daily_digest(analyzed_contents)
        
        # Configuration snapshot pour l'historique
        config_snapshot = {
            'profile': profile,
            'environment': environment,
            'collection_config': {
                'total_limit': config.collection.total_limit,
                'keywords': config.collection.keywords
            },
            'analysis_config': {
                'expert_level': config.analysis.expert_level,
                'recommendation_threshold': config.analysis.recommendation_threshold
            },
            'synthesis_config': synthesis_config,
            'cache_settings': {
                'max_age_hours': cache_max_age_hours,
                'skip_cache': skip_cache
            },
            'execution_time': start_time.isoformat()
        }
        
        # Traitement avec historique via service enrichi
        synthesis_result = await integration_service.process_synthesis_with_history(
            analyzed_articles,
            synthesize_content,
            config_snapshot
        )
        
        daily_digest = synthesis_result['daily_digest']
        
        # Sauvegarde du digest selon la config output
        output_path = await synthesizer.save_digest_to_file(
            daily_digest, 
            output_dir=config.output.reports_dir
        )
        
        logger.info(f"‚úÖ Digest avec historique g√©n√©r√©:")
        logger.info(f"   üìã {daily_digest.title}")
        logger.info(f"   üìÑ {daily_digest.word_count} mots ({daily_digest.estimated_read_time}min)")
        logger.info(f"   üèÜ {len(daily_digest.top_articles)} articles vedettes")
        logger.info(f"   üí° {len(daily_digest.key_insights)} insights cl√©s")
        logger.info(f"   üéØ {len(daily_digest.recommendations)} recommandations")
        logger.info(f"   üíæ Sauvegard√©: {output_path}")
        logger.info(f"   üóÑÔ∏è Historique BD: ID {synthesis_result['digest_id']}")
        
        # ===============================
        # M√âTRIQUES ET PERFORMANCE
        # ===============================
        logger.info("\nüìä Sauvegarde des m√©triques de performance...")
        
        # Mise √† jour des statistiques de session
        integration_service.session_stats['articles_processed'] = len(collection_result.contents)
        
        # Sauvegarde des m√©triques via service enrichi
        metrics_id = integration_service.save_session_metrics(
            collection_result,
            analyzed_articles,
            daily_digest
        )
        
        # ===============================
        # R√âSUM√â FINAL ENRICHI
        # ===============================
        total_time = (datetime.now() - start_time).total_seconds()
        
        logger.info(f"\nüéâ DIGEST QUOTIDIEN ENRICHI G√âN√âR√â AVEC SUCC√àS!")
        logger.info(f"‚è±Ô∏è Temps total: {total_time:.2f}s")
        logger.info(f"üìä Performance: {total_time/len(analyzed_articles):.2f}s/article")
        logger.info(f"üìÑ Fichier: {output_path}")
        logger.info(f"üóÑÔ∏è M√©triques BD: ID {metrics_id}")
        
        # Aper√ßu du contenu
        logger.info(f"\nüìã APER√áU DU DIGEST ENRICHI:")
        logger.info(f"üó∫Ô∏è {daily_digest.title}")
        
        # Top articles
        for i, article in enumerate(daily_digest.top_articles[:2], 1):
            logger.info(f"   {i}. {article.title_refined}")
            logger.info(f"      üìä Score: {article.relevance_for_audience:.2f} | {article.complexity_level}")
        
        # Top insights
        for insight in daily_digest.key_insights[:2]:
            logger.info(f"   üí° {insight}")
        
        # Statistiques enrichies
        logger.info(f"\nüìà STATISTIQUES ENRICHIES:")
        logger.info(f"   üîÑ Taux d√©duplication: {dedup_stats['duplication_rate']:.1%}")
        logger.info(f"   üíæ Cache hits: {integration_service.session_stats['cache_hits']}")
        logger.info(f"   ‚è±Ô∏è Temps √©conomis√©: {integration_service.session_stats['analysis_time_saved']:.1f}s")
        
        # Affichage du r√©sum√© quotidien
        integration_service.print_daily_summary()
        
        return {
            'digest': daily_digest,
            'output_path': output_path,
            'stats': {
                'collection': collection_result,
                'deduplication': dedup_stats,
                'analysis': analyzed_articles,
                'synthesis': synthesis_result,
                'total_time': total_time
            },
            'db_ids': {
                'digest_id': synthesis_result['digest_id'],
                'metrics_id': metrics_id
            }
        }
        
    except Exception as e:
        logger.error(f"‚ùå Erreur lors de l'ex√©cution enrichie: {e}")
        logger.exception("D√©tails de l'erreur:")
        raise


def main():
    """Fonction principale enrichie avec gestion des arguments CLI Phase 3."""
    
    # Parse des arguments
    args = parse_arguments()
    
    # Configuration du logging
    log_level = "DEBUG" if args.verbose else args.log_level
    setup_logging(log_level)
    
    # D√©termination du profil et environnement
    profile = args.profile or ("demo" if args.demo else None)
    environment = args.environment
    
    # Options Phase 3
    cache_max_age = 0 if args.skip_cache else args.cache_max_age
    skip_cache = args.skip_cache
    db_path = args.db_path
    
    # Pr√©paration des overrides
    overrides = {}
    if args.total_limit:
        overrides['total_limit'] = args.total_limit
    if args.target_audience:
        overrides['target_audience'] = args.target_audience  
    if args.max_articles:
        overrides['max_articles'] = args.max_articles
    
    # Log des param√®tres d'ex√©cution enrichie
    logger.info("üöÄ Agent de Veille Intelligente ENRICHI - Phase 3")
    logger.info("üóÑÔ∏è Version avec Base de Donn√©es Avanc√©e")
    if profile:
        logger.info(f"üìã Profil: {profile}")
    if environment:
        logger.info(f"üåç Environnement: {environment}")
    if overrides:
        logger.info(f"üîß Overrides: {overrides}")
    
    # Options Phase 3
    logger.info(f"üíæ Cache: {'D√âSACTIV√â' if skip_cache else f'{cache_max_age}h max'}")
    if db_path:
        logger.info(f"üóÑÔ∏è BD personnalis√©e: {db_path}")
    
    try:
        # Actions pr√©liminaires Phase 3
        if args.cleanup_old:
            logger.info("üßπ Nettoyage des anciennes donn√©es...")
            integration_service = VeilleIntegrationService(db_path)
            cleanup_result = integration_service.cleanup_old_data()
            logger.info(f"‚úÖ Nettoyage termin√©: {cleanup_result}")
        
        if args.show_stats:
            logger.info("üìä Affichage des statistiques de la BD...")
            integration_service = VeilleIntegrationService(db_path)
            integration_service.print_daily_summary()
            return
        
        # Ex√©cution principale enrichie
        result = asyncio.run(create_daily_digest_enhanced(
            profile=profile,
            environment=environment,
            db_path=db_path,
            cache_max_age_hours=cache_max_age,
            skip_cache=skip_cache,
            **overrides
        ))
        
        if result:
            logger.info("üéâ Traitement enrichi termin√© avec succ√®s!")
            
            # Affichage des statistiques finales enrichies
            stats = result['stats']
            db_ids = result['db_ids']
            
            logger.info(f"üìä Statistiques finales enrichies:")
            logger.info(f"   üì° Collect√©s: {stats['collection'].total_collected}")
            logger.info(f"   üîÑ Doublons √©vit√©s: {stats['deduplication']['duplicates_removed']}")
            logger.info(f"   üß† Analys√©s: {len(stats['analysis'])}")
            logger.info(f"   ‚è±Ô∏è Dur√©e totale: {stats['total_time']:.1f}s")
            logger.info(f"   üìÑ Fichier: {result['output_path']}")
            logger.info(f"   üóÑÔ∏è BD - Digest ID: {db_ids['digest_id']}, M√©triques ID: {db_ids['metrics_id']}")
        else:
            logger.error("‚ùå √âchec du traitement enrichi")
            sys.exit(1)
            
    except KeyboardInterrupt:
        logger.warning("‚ö†Ô∏è Interruption utilisateur")
        sys.exit(130)
    except Exception as e:
        logger.error(f"‚ùå Erreur fatale enrichie: {e}")
        if log_level == "DEBUG":
            logger.exception("D√©tails:")
        sys.exit(1)


if __name__ == "__main__":
    main()
